\section{Introduction}
The motivation for creating a package for the modelling of smooth transition
ARMA models was the observation that many phenomena appear to go
through different states which may be explained by some underlying and
observable factors.\footnote{As opposed to unobservable factors which leads to
a different class of models, such as the Markov switching models.}
In financial markets, under different states of the business cycle, financial
instruments have been observed to exhibit different characteristics with
recessions (or the lead up to such) usually marked by increased volatility and lower 
or negative mean. Being able to model the evolution of the process driving such 
changes and hence the switch from one state to another must surely make for a 
better understanding of the underlying dynamics and perhaps lead to a better
forecast model.

The precursor to smooth transition models appears to have been
\cite{Carmichael1928} who posited the use of the arctangent transformation
and even considered the possibility of a double transition well before the
plethora of papers which came more than 30 years later. While \cite{Quandt1958}
originally discussed a switching regression model, the pioneering contributions
to the literature on more general threshold autoregression (TAR) have
been \cite{Tong1980} and \cite{Tong1981}, with a more general class of nonlinear
AR models introduced in a series of papers by \cite{Billings1983}, \cite{Billings1986},
and \cite{Zhu1993}. Many extensions to the basic TAR model have been considered
in the literature , including smooth transition based on the Gaussian CDF in
\cite{ChanTong1986}, the more widely adopted logistic (popularized by \cite{May1976} 
and discussed in \cite{Tong1990}) (LSTAR)
and exponential (ESTAR) models discussed in \cite{Terasvirta1994}, double transitions 
in \ldots\cite{}, nested transition in \cite{Astatkie1997},
multiple states in \cite{Dijk1999}, and the inclusion of GARCH dynamics in
\citeauthor{Chan2002}(\citeyear({Chan2002}), \citeyear{Chan2003}). A review of
recent extensions and the state of research in this area can be found in \cite{Dijk2002}.

Common themes among the vast majority of research in the area of smooth
transition AR models are the use of the self-exciting model, where the lagged
value of the dependent variable (or some simple transformation of the same) is
used and a representation for the state dynamics which appears overly
restrictive.
The twinkle package departs from the traditional representation which appears to
have dominated research in the area of STAR models and reparameterizes the state
dynamics to include a possible linear combination of multivariate
variables and the use of autoregressive first order dynamics. The m-states model
is also reparameterized to more closely resemble the representation of the
multinomial logistic regression model in the way the probabilities are summed
and weighted across states. Further extensions include a 2-state AR mixture
model partially bridging the gap with the finite mixture models, the inclusion of 
GARCH dynamics, MA dynamics either inside or outside the states, and a large
number of conditional distributions which follow from the rugarch package. Methods 
for model specification, estimation, filtering, forecasting and simulation are
provided with similar interface and access methods as in related packages by the author. 
It is important for the interested user to be aware from the start that such
models are difficult to estimate, may contain local minima and may be generally
hard to solve with confidence. While the package has made efforts to provide for a 
number of solvers and strategies to estimate these models, confident estimation
may prove challenging depending on properties of the dataset used and choice of 
model options. The model is naturally greedy in requiring a substantial amount of data to
confidently identify the optimal classification of states given the conditional
mean equation. From experience, it is this author's opinion that these types of
models may not be as forgiving as linear models when it comes to forecasting, 
depending on whether the actual forecast state contains the type of
nonlinearities under which the model was estimated. Thus, unlike linear
models, the misclassification of the forecast state may be more costly.

This paper is organized as follows: Section \ref{sec:1} discusses the
representation of the model in the twinkle package and how the model can be
specified. Section \ref{sec:2} discusses forecasting with a special emphasis on
the different methods implemented for n-period ahead forecasts, followed by
Section \ref{sec:3} on the simulation. Finally, Section \ref{sec:4} presents a
number of examples using real and simulated data.
While every possible effort has been made to test the model and its methods
under different scenarios and squash any bugs, the package is still quite new
and further testing is required. General questions on the package should be
posted to the R-SIG-FINANCE mailing list, while bugs (with reproducible code
and preferably a patch) and suggestions can be reported directly to me.
Finally I would like to acknowledge the valuable help of Eduardo Rossi who
collaborated on the new representation and research publication in this area.
\section{Smooth Transition ARMA Models Revisited}\label{sec:1}
\subsection{Dynamics and Extensions}
Consider the standard representation of a STAR model (adapted from
\cite{Dijk1999})
\begin{equation}\label{eq:star_original}
{y_t} = {{\phi '}_1}y_t^{\left( p \right)}\left( {1 - F\left( {{z_t};\gamma
,\alpha ,c} \right)} \right) + {{\phi '}_2}y_t^{\left( p \right)}\left( {1 - F\left( {{z_t};\gamma ,\alpha ,c} \right)} \right) + {\varepsilon _t}
\end{equation}
where $y_t^{\left( p \right)} = {\left( {1,\tilde y_t^{\left( p
\right)}}\right)^\prime }$, $\tilde y_t^{\left( p \right)} = {\left(
{{y_{t-1}},\dots,{y_{t - p}}} \right)^\prime }$,${\phi _i} = {\left( {{\phi
_{i0}},{\phi _{i1}},\dots,{\phi _{ip}}} \right)^\prime }$, $i = 1,2$ and
$\varepsilon_t$ is a white noise zero mean error process with standard deviation
$\sigma$. The state transition function $F\left( {{z_t};\gamma ,\alpha ,c} \right)$ 
is a continuous function bounded on the unit interval and usually taken to be
the logistic CDF\footnote{At present only the Logistic STAR model is
entertained and it is not likely that the exponential STAR model will be
considered at all.} such that:
\begin{equation}\label{eq:logistic_cdf}
F\left( {{z_t};\gamma ,\alpha ,c} \right) = {\left( {1 + \exp \left\{ { -
\gamma \left( {\alpha '{z_t} - c} \right)} \right\}} \right)^{ - 1}},\gamma  > 0
\end{equation}
where ${z_t} = {\left( {{z_{1t}},\dots,{z_{jt}}} \right)^\prime },j = 1,\dots,k$
is a vector of $k$ observed variables which are assumed to explain the state
transition. These can be a set of explanatory variables or the lagged values of $y_t$ in which case
the model is called 'self-exciting'. It is also possible that the variable is
time in which case the model can be used to identify breaks in the mean as in 
\cite{Lin1994}, or a combination of time and other variables giving rise to the
time varying STAR (TVSTAR) model discussed in \cite{Lundbergh2003}. As correctly 
noted by \cite{Dijk1999}, the vector of parameters $\alpha$ needs to be
nornalized in some way in order to achieve identification (i.e by setting
$\alpha_1$=1). The parameter $\gamma$ is then a type of scaling factor which
determines the smoothness (or speed) of the transition, with values at the limits, 
$\left[ {0,\infty } \right]$, representing linear and SETAR type transitions 
respectively. By far the most popular test of STAR nonlinearity is described 
in \cite{Luukkonen1988} using a Taylor series expansion around
equation \ref{eq:logistic_cdf}, effectively testing whether $\gamma=0$ (via
a auxilliary regression), which would in turn imply that the $\alpha$ vector is
also zero and hence in favour of linearity. 
However, there is really little reason for estimating $\gamma$ seperately in the
STAR model since we can allow it to be subsumed by the vector of state 
parameters $\left(\alpha, c \right)$. In doing so we also gain the additional
advantage of extending the type of dynamics to include autoregression as
follows:
\begin{equation}\label{eq:star_new}
\begin{gathered}
  F\left( {{z_t};\alpha ,c,\beta} \right) = {\left( {1 + \exp \left\{ -{{\pi
  _t}} \right\}} \right)^{ - 1}} \hfill \\
  {\pi _t} = \hat c + \hat \alpha '{z_t} + \beta '\pi _t^{\left( q \right)} \hfill \\
  \pi _t^{\left( q \right)} = {\left( {{\pi _{t - 1}},\dots,{\pi _{t - q}}}
  \right)^\prime } \hfill \\
\end{gathered}
\end{equation}
where the unconstrained state dynamics $\pi_t$ can be initialized by setting
${\pi _0} =\frac{{\hat c + \hat \alpha 'E\left[ z \right]}}{{1 - \beta
'{\mathbf{1}}}}$ which requires that $\left| {\beta '{\mathbf{1}}} \right| < 1$.
It should be  clear from this representation that  $\hat c = \gamma c,\hat
\alpha ' = \gamma {\left( {1,{\alpha _2},\dots,{\alpha _j}} \right)^\prime },j =
1,\dots,k$ recovers the  original representation in equation
\ref{eq:star_original}. The use of  autoregressive dynamics in the state
equation follows related work in the  area of dynamic binary response models of 
\cite{Kauppi2008} and \cite{Nyberg2010}.
Generally, estimation becomes quite difficult for more than one autoregressive
parameter in the state dynamics which is why at present only a lag-1
autoregressive model is allowed in the package.

The conditional mean dynamics are not limited to AR terms but may include
external regressors (ARX) and moving average (MA) terms in the states giving
rise to a full STARMAX model specification:
\begin{equation}\label{eq:starmax}
{y_t} = \left( {{{\phi '}_1}y_t^{\left( p \right)} + {{\xi '}_1}{x_t} + {{\psi
'}_1}e_t^{\left( q \right)}} \right)\left( {1 - F\left( {{z_t};\alpha ,c,\beta } \right)} \right) + \left( {{{\phi '}_2}y_t^{\left( p \right)} + {{\xi '}_2}{x_t} + {{\psi '}_2}e_t^{\left( q \right)}} \right)\left( {1 - F\left( {{z_t};\alpha ,c,\beta } \right)} \right) + {\varepsilon _t}
\end{equation}
where $\varepsilon _t^{\left( q \right)} = {\left( {{\varepsilon _{t -
1}},\dots,{\varepsilon _{t - q}}} \right)^\prime },{{\psi '}_i} = {\left( {{\psi
_{i1}},\dots,{\psi _{iq}}} \right)^\prime }$ represent the $q$ moving average
terms and parameters per state $i$ ($i=1,2$), and ${x_t} = {\left(
{{x_1},\dots,{x_l}} \right)^\prime },{{\xi '}_1} = {\left( {{\xi
_{i1}},\dots,{\xi _{il}}} \right)^\prime }$ the $l$ external regressors and
their parameters per state $i$. It is also possible that the MA term enters
outside of the states instead of inside giving rise to a STARX with Linear MA
terms (STARXLMA):
\begin{equation}
{y_t} = \left( {{{\phi '}_1}y_t^{\left( p \right)} + {{\xi '}_1}{x_t}}
\right)\left( {1 - F\left( {{z_t};\alpha ,c,\beta } \right)} \right) + \left(
{{{\phi '}_2}y_t^{\left( p \right)} + {{\xi '}_2}{x_t}} \right)\left( {1 -
F\left( {{z_t};\alpha ,c,\beta } \right)} \right) + \psi 'e_t^{\left( q \right)}
+{\varepsilon _t}
\end{equation}
The STARMAX model therefore encompasses a very wide range of sub-models based on
the type of restrictions placed in the conditional mean and state dynamics, and
choice of switching variables in the latter.\\
A natural question which arises from the representation is whether it is
reasonable to assume that the conditional variance is the same in both states.
Re-write the 2-state STARX equation as follows:
\begin{equation}
{\varepsilon _t} = {y_t} - {p_t}\left( {{\mu _{1t}}} \right) - \left( {1 -
{p_t}} \right)\left( {{\mu _{2t}}} \right)
\end{equation}
where $\mu_{i1}$ and $\mu_{2,t}$ represent the conditional mean dynamics per
state at time $t$ and $p_t$ the conditional probability. Add and subtract
${p_t}{y_t}$, and re-arrange:
\begin{equation}\label{eq:starmix}
\begin{gathered}
  {\varepsilon _t} =  {\color{blue}+ {p_t}{y_t}} - {p_t}\left( {{\mu _{1t}}}
  \right) + {y_t} {\color{blue} - {p_t}{y_t}} - \left( {1 - {p_t}} \right)\left(
  {{\mu _{2t}}} \right) \hfill \\
  {\varepsilon _t} = {p_t}{y_t} - {p_t}\left( {{\mu _{1t}}} \right) + {y_t}\left( {1 - {p_t}} \right) - \left( {1 - {p_t}} \right)\left( {{\mu _{2t}}} \right) \hfill \\
  {\varepsilon _t} = {p_t}\left( {{y_t} - {\mu _{1t}}} \right) + \left( {1 - {p_t}} \right)\left( {{y_t} - {\mu _{2t}}} \right) \hfill \\
  {\varepsilon _t} = {p_t}\left( {{\varepsilon _{1,t}}} \right) + \left( {1 - {p_t}} \right)\left( {{\varepsilon _{2,t}}} \right) \hfill \\
  {\varepsilon _{1,t}} \sim N\left( {0,\sigma _1^2} \right) \hfill, {\varepsilon
  _{2,t}} \sim N\left( {0,\sigma _2^2} \right) \hfill \\
  {\varepsilon _t} \sim N\left( {0,{p_t}\sigma _1^2 + \left( {1 - {p_t}}
  \right)\sigma _2^2} \right) \hfill \\
\end{gathered}
\end{equation}
Thus, the model can naturally be re-formulated as a mixture of Normals with
mixing probabilities derived from the state dynamics. This provides for a more
parsimonious and clear extension than using GARCH dynamics on the mixed state
residuals. Alternatively, it can be thought of as the time-invariant version of
a STARX-STGARCH model where the STARX and STGARCH models have common transition
dynamics. This also provides a partial bridge between finite mixture and
time-series autoregressive models.

\subsection{Multiple States}
\cite{Dijk1999} consider extending the 2-state STAR model in equation
\ref{eq:star_original} to a 4-state models as follows:
\begin{equation}\label{eq:mrstar_original}
\begin{aligned}
{y_t} &=& \left[ {{{\phi '}_1}y_t^{\left( p \right)}\left( {1 - F\left(
{{z_t};{\gamma _1},\alpha ,c} \right)} \right) + {{\phi '}_2}y_t^{\left( p
\right)}\left( {1 - F\left( {{z_t};{\gamma _1},\alpha ,c} \right)} \right)}
\right]\left( {1 - F\left( {{z_t};{\gamma _2},b,d} \right)} \right)\\
&+& \left[ {{{\phi '}_3}y_t^{\left( p \right)}\left( {1 - F\left(
{{z_t};{\gamma _1},\alpha ,c} \right)} \right) + {{\phi '}_4}y_t^{\left( p \right)}\left( {1 -
F\left( {{z_t};{\gamma _1},\alpha ,c} \right)} \right)} \right]F\left(
{{z_t};{\gamma _2},b,d} \right) + {\varepsilon _t}\\
\end{aligned}
\end{equation}
Alternatively, the implementation followed in this package takes a
page out of multinomial regression and models multiple states using the
following representation:
\begin{equation}\label{eq:mrstar_new}
{y_t} = \sum\limits_{i = 1}^s {\left[ {\left( {{{\phi '}_i}y_t^{\left( p \right)} + {{\xi '}_i}{x_t} + {{\psi '}_i}e_t^{\left( q \right)}} \right){F_i}\left( {{z_t};{\alpha _i},{c_i}} \right)} \right]}  + {\varepsilon _t}
\end{equation}
with
\begin{equation}
\begin{gathered}
  {F_i}\left( {{z_t};{\alpha _i},{c_i}} \right) = \frac{{{e^{{\pi _{i.t}}}}}}
{{1 + \sum\limits_{i = 1}^{s - 1} {{e^{{\pi _{i,t}}}}} }} \hfill \\
  {F_s}\left( {{z_t};{\alpha _i},{c_i}} \right) = \frac{1}
{{1 + \sum\limits_{i = 1}^{s - 1} {{e^{{\pi _{i,t}}}}} }} \hfill \\ 
\end{gathered}
\end{equation}
where the $s$ states are weighted to sum to unity. This appears, at least to
this author, to be a more natural representation for a multi-state setup.

\subsection{Estimation}
Estimation of the STARMAX models is done by maximizing the likelihood without
imposing any particular inequality restrictions on the state dynamic intercepts
or any parameter bound restrictions (except for positivity bounds on the
variance).
Since unconstrained optimizers appear to do quite well for hard nonlinear/non-smooth problems, the main solver in the twinkle 
package is BFGS from the optim function. It is possible to include parameter
bounds in which case a logistic-transformation is used with the unconstrained solvers.
Additional solvers included are 'nlminb', 'solnp', 'cmaes' and 'deoptim'.
However, it is suggested that either a multi-start strategy is followed
(by choosing 'msoptim') or an iterative search strategy ('strategy') which
cycles between fixing the state parameters to and estimate the conditional mean 
parameters (linear), fixing the conditional mean parameters to estimate the
state parameters (nonlinear) and a random start estimation. As with general
nonlinear optimization problems, scaling of the variables prior to estimation
may help, an ica or pca transformation if they are highly correlated, or hinge
basis transformation of the dataset via the earth package is another interesting
option in the case of relevant feature extraction.
\section{Forecasting}\label{sec:2}
Consider a general nonlinear first order autoregressive model:
\begin{equation}\label{eq:nonlinar1}
{y_t} = F\left( {{y_{t - 1}};\theta } \right) + {\varepsilon _t}
\end{equation}
where $ F\left( {{y_{t - 1}};\theta } \right)$ is some nonlinear function 
mapping $y_{t-1}$ to $y_t$ given the parameter set $\theta$. The optimal h-step
ahead  point forecast, using a least squares criterion, of $y_{t+h}$ at time $t$
is given by:
\begin{equation}
{{\hat y}_{t + h\left| t \right.}} = E\left[ {{y_{t + h}}\left| {{\Im _t}} \right.} \right]
\end{equation}
where $\Im_t$ is the information set upto time $t$. Given  that $E\left[
{{\varepsilon_{t + 1}}\left| {{\Im _t}} \right.} \right] = 0$, then the
1-step-ahead optimal forecast is:
\begin{equation}
{{\hat y}_{t + 1\left| t \right.}} = E\left[ {{y_{t + 1}}\left| {{\Im _t}} \right.} \right] = F\left( {{y_t};\theta } \right)
\end{equation}
which is the same as when $F(.)$ is linear. However, for horizons greater than
1, this is not the case since $E\left[ {F\left( . \right)} \right] \ne F\left(
{E\left[ . \right]} \right)$,  which means that simple recursive relationship 
found in the linear case do not exist in the nonlinear case. Instead, consider
the  h-step-ahead point forecast using the following closed form 
representation:\footnote{This is based on the Chapman-Kolmogorov relation:
\begin{equation}
g\left( {{y_{t + h}}\left| {{\Im _t}} \right.} \right) = \int_{ - \infty }^\infty  {g\left( {{y_{t + h}}\left| {{y_{t + h - 1}}} \right.} \right)g\left( {{y_{t + h - 1}}\left| {{\Im _t}} \right.} \right)d{y_{t + h - 1}}}
\end{equation}
which leads to Equation \ref{eq:hstepforecast} after taking conditional expectations from both sides.
}
\begin{equation}\label{eq:hstepforecast}
E\left[ {{y_{t + h}}\left| {{\Im _t}} \right.} \right] = \int_{ - \infty }^\infty  {E\left[ {{y_{t + h}}\left| {{y_{t + h - 1}}} \right.} \right]g\left( {{y_{t + h - 1}}\left| {{\Im _t}} \right.} \right)d{y_{t + h - 1}}}
\end{equation}
where $g\left( {{y_{t + h}}\left| {{\Im _t}} \right.} \right) = f\left( {{y_{t + h}} - F\left( {{y_{t + h - 1}};\theta } \right)} \right)$, is the distribution of the shock $\varepsilon_{t+h}$ with mean $F\left( {y_{t + h - 1}}\right)$, though the distribution $\varepsilon_t$ is never known with certainty.
A number of approaches have been used in the literature to estimate this
integral. It is simple to see that the conditional distribution of ${g\left(
{{y_{t + h - 1}}\left| {{\Im _t}} \right.} \right)}$ can be obtained recursively
starting at h=2 and noting that $g\left( {{y_{t + 1}}\left| {{\Im _t}} \right.}
\right) = f\left( {{y_{t + 1}} - F\left( {{y_t};\theta } \right)} \right)$. To
obtain the forecasts, numerical integration can be used (applied recursively) or
monte carlo methods. In the former case, the form of the conditional
distribution $f\left( {.\left| {{\Im _t}} \right.} \right)$ can be replaced by a
kernel estimator, whereas in the latter case one has an option of using an
empirical  bootstrap, simulating from the conditional distribution $f\left(
{.\left| {{\Im _t}} \right.} \right)$ or a kernel estimator.  For instance,  
the 2-step ahead monte carlo forecast is given by:
\begin{equation}
{{\hat y}_{t + 2\left| t \right.}} = \frac{1}{T}\sum\limits_{i = 1}^T {F\left( {{{\hat y}_{t + 1\left| t \right.}} + {\varepsilon _i};\theta } \right)}
\end{equation}
However, in the case when a GARCH model is used for the modelling of the
conditional variance, then the monte carlo forecast needs to be adjusted as follows:
\begin{equation}
{{\hat y}_{t + 2\left| t \right.}} = \frac{1}{T}\sum\limits_{i = 1}^T {F\left( {{{\hat y}_{t + 1\left| t \right.}} + {z_i}{{\hat \sigma }_{t + 2\left| t \right.}};\theta } \right)}
\end{equation}
where $z_i$ represent draws from either the parametric standardized distribution
of the model  or the standardized in-sample innovations (or draws from a kernel
estimated density of the standardized in-sample innovations), which are then
multiplied  by the forecast GARCH volatility ${{{\hat \sigma }_{t + 2\left| t
\right.}}}$ to obtain the forecast residuals $\varepsilon_i$.
One benefit of using a monte-carlo or bootstrap approach is that they
immediately give rise to the density of each point forecast thus allowing  for
the creation of interval forecasts.

\section{Simulation}\label{sec:3}

\section{Generalized Impulse Response}\label{sec:4}

\pagebreak
\section{Software Implementation}\label{sec:5}
The entry point to defining and estimating a STARMAX model in the twinkle
package is the starspec function:
\begin{lstlisting}
>starspec
function(
mean.model = list(states=2, include.intercept=c(1,1), arOrder=c(1,1), 
	maOrder=c(0, 0), matype="linear", statevar=c("y","x"), x=NULL, 
	statear=FALSE, statelags=1, external.regressors=NULL, yfun=NULL, 
	transform="log"), 
variance.model=list(dynamic=FALSE, model="sGARCH", garchOrder=c(1,1), 
	submodel=NULL, external.regressors=NULL, 
	variance.targeting=FALSE), 
distribution.model="norm", start.pars=list(), fixed.pars=list(), 
fixed.prob=NULL, ...)
}
\end{lstlisting}
The \textbf{mean.model} defines the equation for the conditional mean dynamics
including the state dynamics. Upto 4 \textbf{states} are allowed, with the
1-state option having a special implementation in that the \textbf{fixed.probs}
list is an xts matrix (alined to the dataset which will be passed to the
estimation routine) of weights. By default this is set to a vector of ones in
this case but may be any other 'time-weighting' scheme the user  wishes. The
options for \textbf{intercept}, \textbf{arOrder} and \textbf{maOrder} should be
integer vectors of length equal to the number of \textbf{states}.The
\textbf{matype} denotes whether the moving average terms enters inside the
states ('states') or outside ('linear'). The \textbf{statevar} indicates whether
the model will switch based on its own value ('y') or an exogenouse set of
regressors ('x'), in which case an xts matrix (aligned to the index of
the dataset) is passed to \textbf{x}. This matrix must \emph{NOT} be lagged,
instead use the \textbf{statelags} option to pass an integer vector of lags (of length equal to
the number of columns of x).\footnote{The decision to enforce this choice was
motivated by the need to control the way the state equation is initialized so
that it uses all available information at any given time without waiting for
all lags to be in play before being used (i.e. the typical use of
starting the recursion at max(lags) is completely wasteful.)}
In the case that \textbf{statevar} is 'y', then \textbf{statelags} can be an 
integer vector of the unique lags to use as a linear combination. The
\textbf{yfun}  option allows the user to pass a function to transform the value 
of y\footnote{The function must return the same length as the value it receives
without any NAs or NaNs.} prior to being used in the state dynamics equation.
While it may appear at first that the same can be achieved by passing a
pre-transformed value and using 'x' as the \textbf{statevar}, consider that
simulation and n-ahead forecasts (which depend on simulation methods) on
transformed values of 'y' can then be used directly, where it would have been
impossible to do so otherwise. Unlike the \textbf{x} option, the
\textbf{external.regressors} needs to be passed pre-lagged (again, as an xts
matrix aligned to the index of the dataset). The \textbf{statear} indicates
whether to include lag-1 autoregression in the state dynamics as discussed
previously in equation \ref{eq:star_new}.
Finally, the \textbf{transform} is currently fixed to use only the logistic
transformation, and there are no plans to extend to the exponential at present.
The variance model can be \textbf{dynamic}, in which case a choice of
'mixture','sGARCH','gjrGARCH' and 'eGARCH' are implemented. For the GARCH
flavors, the rest of the options follow from the rugarch package, whilst the
'mixture' model is based on equation \ref{eq:starmix}. All distributions
implemented in rugarch are included as options in \textbf{distribution.model},
while fixed and starting parameters can be passed directly via
\textbf{fixed.pars} and \textbf{start.pars} respectively, else later on via the
\textbf{setfixed<-} and \textbf{setpars<-} methods on the star specification
(note that there is also a \textbf{setbounds<-} methods for setting and
enforcing parameter bounds). Finally, the \textbf{fixed.probs} list allows the
user to pass an xts matrix of fixed probabilities for each state (aligned to
the index of the dataset and with columns equal to \textbf{states}). This could
for instance be the forecast probabilities from another model (e.g. logistic
regression) representing market up and down periods, recessions etc. In this
case the state equation is not used and the model is effectively linear and
extremely fast to estimate for the conditional mean dynamics.

\section{Examples}\label{sec:6}
